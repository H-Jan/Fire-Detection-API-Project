{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ _DeepFire_: API Project for Fire Detection ðŸ”¥\n",
    "\n",
    "In this project, you'll apply your skills at neural network development in a new way: taking a model that you've trained yourself and deploying it to a static webpage that you can work with to upload new images and get prediction accuracy results. \n",
    "\n",
    "This project will primarily focus on your abilities in creating and testing neural network architecture development. \n",
    "\n",
    "#### **Specifically, you'll be creating a convolutional neural network that can ingest Fire Detection Image Data and predict binary class values, similarly to what we've done with multilayer perceptrons in the past.**\n",
    "\n",
    "Boilerplate and supporting architectures have been provided for a multitude of tasks ranging from data preprocessing, processing, ingestion, and predictive assessment â€“Â however, major tasks and design work will ultimately be left to you to approach and figure out ideal, optimized solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ General Importations\n",
    "\n",
    "As always, we'll start with importing basic tools and functions for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import utils\n",
    "\n",
    "import os, PIL\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Initializing Deep Learning Tools ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Your first task will be crucial to ensuring the successful implementation of the rest of your notebook. \n",
    "> \n",
    "> **Initialize each line with the correct function type from the TensorFlow documentation.**\n",
    "> \n",
    "> Feel free to refer throughout the notebook and across previous notebooks to see which TensorFlow architectures you've used for similar tasks. \n",
    "> \n",
    "> To give you a guide for how this should look, you've been provided with a single correct function declaration in the form of `image_dataset_from_directory` at the end of the cell. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO's\n",
    "\"\"\" Sequential Model Architecture \"\"\"\n",
    "Sequential = tf.keras.models.Sequential\n",
    "\n",
    "\"\"\" Data Preprocessing Functions \"\"\"\n",
    "Resizing = tf.keras.layers.Resizing\n",
    "Rescaling = tf.keras.layers.Rescaling\n",
    "\n",
    "\"\"\" Data Augmentation Functions \"\"\"\n",
    "RandomFlip = tf.keras.layers.RandomFlip\n",
    "RandomRotation = tf.keras.layers.RandomRotation\n",
    "RandomZoom = tf.keras.layers.RandomZoom\n",
    "\n",
    "\"\"\" Artificial Neural Network Layer Inventory \"\"\"\n",
    "Dense = tf.keras.layers.Dense\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "\n",
    "\"\"\" Convolutional Neural Network Layer Inventory \"\"\"\n",
    "Conv2D = tf.keras.layers.Conv2D\n",
    "MaxPool2D = tf.keras.layers.MaxPool2D\n",
    "Flatten = tf.keras.layers.Flatten\n",
    "\n",
    "\"\"\" Residual Network Layer Inventory \"\"\"\n",
    "ResNet50 = tf.keras.applications.resnet50.ResNet50\n",
    "\n",
    "\"\"\" Function to Load Images from Target Folder \"\"\"\n",
    "image_dataset_from_directory = tf.keras.preprocessing.image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Precheck Image Dataset Sizes\n",
    "\n",
    "If you've followed instructions carefully from the `project/PROJECT.md` instructions, the following dataset directory instantiations should work perfectly. \n",
    "\n",
    "If they do not, double-check to make sure you've saved your dataset to the appropriate location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of fire image samples: 541\nNumber of non-fire image samples: 541\n"
     ]
    }
   ],
   "source": [
    "# Use the `glob.glob` function to show how many images are in each folder\n",
    "DATA_DIRECTORY = \"./dataset/Images/\"\n",
    "FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Fire_Images/*\"\n",
    "NOT_FIRE_IMAGES_PATTERN = f\"{DATA_DIRECTORY}/Normal_Images/*\"\n",
    "\n",
    "print(f\"Number of fire image samples: {len(glob(FIRE_IMAGES_PATTERN))}\")\n",
    "print(f\"Number of non-fire image samples: {len(glob(NOT_FIRE_IMAGES_PATTERN))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¸ Load Dataset\n",
    "\n",
    "Like we've done previously, let's set our batch size and image dimensions to work seamlessly with our configured model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1082 files belonging to 2 classes.\n",
      "Using 866 files for training.\n",
      "2021-09-30 19:50:24.393642: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1082 files belonging to 2 classes.\nUsing 216 files for validation.\n"
     ]
    }
   ],
   "source": [
    "validation = image_dataset_from_directory(\n",
    "    directory=DATA_DIRECTORY,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can actually see that we have a major class imbalance with our fire images representing our minority class. \n",
    "\n",
    "Let's go ahead and fix that by resampling our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Resample (Oversample) Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_class_distribution(train, DATA_DIRECTORY=DATA_DIRECTORY, save=True):\n",
    "    \"\"\" Helper function to resample class distribution for image dataset. \"\"\"\n",
    "    minority_class, majority_class = list(), list()\n",
    "    for images, labels, in train.take(3):\n",
    "        for image, label in zip(images, labels):\n",
    "            if label == 0:\n",
    "                minority_class.append(image.numpy().astype(np.uint8))\n",
    "            else:\n",
    "                majority_class.append(image.numpy().astype(np.uint8))\n",
    "    FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Fire_Images/*\"))\n",
    "    NOT_FIRE_SIZE = len(glob(f\"{DATA_DIRECTORY}/Normal_Images/*\"))\n",
    "    upsampled_images = np.array(utils.resample(minority_class, replace=True, \n",
    "                                               n_samples=(NOT_FIRE_SIZE - FIRE_SIZE),\n",
    "                                               random_state=42))\n",
    "    if save == True:\n",
    "        index = 0\n",
    "        for image in upsampled_images:\n",
    "            PATH = f\"{DATA_DIRECTORY}/Fire_Images/new_fire_{index}.png\"\n",
    "            PIL.Image.fromarray(image).save(PATH)\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-30 19:50:31.757755: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "resample_class_distribution(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see that additional images have been generated to balance out both classes prior to predictive modeling.\n",
    "\n",
    "**Go ahead and re-run the `Load Dataset` steps to see new generated dataset changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Pre-Optimize Image File Ingestion\n",
    "\n",
    "This is an accessory step to optimize image data ingestion at the cost of slightly higher memory usage. No modifications are required for this function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_performant_datasets(dataset, shuffling=None):\n",
    "    \"\"\" \n",
    "    Custom function to prefetch and cache stored elements\n",
    "    of retrieved image data to boost latency and performance\n",
    "    at the cost of higher memory usage. \n",
    "    \"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    # Cache and prefetch elements of input data for boosted performance\n",
    "    if not shuffling:\n",
    "        return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    else:\n",
    "        return dataset.cache().shuffle(shuffling).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =         configure_performant_datasets(train, shuffling=1000)\n",
    "validation =    configure_performant_datasets(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Create Resizing and Normalization Layers ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll declare your resizing and normalization layers using the layer architectures that you imported earlier. \n",
    ">\n",
    "> Recall that for this step, we want to accomplish two key tasks: \n",
    "> - Resize all images to the predetermined square image dimensions as indicated by `IMAGE_HEIGHT` and `IMAGE_WIDTH`.\n",
    "> - Scale all images so pixel values are within the range of (0., 1.) rather than the original (0., 255.).\n",
    ">\n",
    "> Additionally, since we're working with colorized image data, we'll want to ensure that our image rescaling/normalization step inputs images as stacks-of-three, since each image channel corresponds to red, green, and blue pixel values. \n",
    ">\n",
    "> As always, refer to previous notebook documentation on image normalization for colorization if you need help.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resizing_layer = Resizing(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "normalization_layer = Rescaling(1./255, input_shape =(IMAGE_HEIGHT, IMAGE_WIDTH, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Architecture Creation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> Now time for the main event! \n",
    "> \n",
    "> Here, you'll be creating and instantiating your model architecture. \n",
    "> \n",
    "> For this assignment, you'll be creating a **Convolutional Neural Network** that can process Fire Detection images for predictive purposes.\n",
    "> \n",
    "> _For this project, you will not be provided guidance as to how to design and implement your CNN architecture._\n",
    "> \n",
    "> Refer to previous notebooks and challenges on CNNs as well as online documentation/resources for how to design CNN models on higher-order images. \n",
    "> \n",
    "> **This is a highly creative step, and there are no wrong answers; however, you will be assessed on your experimentation process and why you choose specific modeling layers, configurations, optimizers, regularizers, and overall design choices.**\n",
    ">\n",
    "> Light boilerplate will be provided to get you started, but as always, use any and all resources at your disposal to finish the job! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the Layers Here as part of better format \n",
    "\n",
    "input_layer = tf.keras.layers.InputLayer(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "\n",
    "conv_layer_1 = Conv2D(32, \n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        input_shape=(32, 32,3)\n",
    "        )\n",
    "\n",
    "#Changes to CNN layer \n",
    "conv_layer_2 = Conv2D(64, \n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation=\"relu\"\n",
    "        )\n",
    "\n",
    "conv_layer_3 = Conv2D(128, \n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation=\"relu\"\n",
    "        )\n",
    "\n",
    "conv_layer_4 = Conv2D(192, \n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation=\"relu\"\n",
    "        )\n",
    "\n",
    "conv_layer_5 = Conv2D(256, \n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        padding='same',\n",
    "        activation=\"relu\"\n",
    "        )\n",
    "\n",
    "pooling_layer_1 = MaxPool2D(pool_size=(2,2), strides=(1,1), padding='same') \n",
    "pooling_layer_2 = MaxPool2D(pool_size=(2,2), padding='same')\n",
    "pooling_layer_3 = MaxPool2D(pool_size=(2,2))\n",
    "pooling_layer_4 = MaxPool2D(pool_size=(2,2))\n",
    "\n",
    "dropout_layer_1 = Dropout(0.25)\n",
    "dropout_layer_2 = Dropout(0.25)\n",
    "dropout_layer_3 = Dropout(0.4)\n",
    "dropout_layer_4 = Dropout(0.5)\n",
    "dropout_layer_5 = Dropout(0.6)\n",
    "dropout_layer_6 = Dropout(0.7)\n",
    "\n",
    "dense_layer_1 = Dense(256, activation=\"relu\")\n",
    "dense_layer_2 = Dense(128, activation=\"relu\")\n",
    "dense_layer_3 = Dense(64, activation=\"relu\")\n",
    "\n",
    "flattening_layer = Flatten()\n",
    "\n",
    "output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "\n",
    "#Resizing\n",
    "resizing_layer = Resizing(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "#Rescaling\n",
    "rescaling_layer = Rescaling(1./255, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "#RandomFlip\n",
    "flipping_layer = RandomFlip(\"horizontal\", input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3))\n",
    "\n",
    "#RandomRotation\n",
    "random_rotating_layer = RandomRotation(0.2)\n",
    "\n",
    "#RandomZoom\n",
    "random_zoom_layer = RandomZoom(0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresizing_1 (Resizing)        (None, 256, 256, 3)       0         \n_________________________________________________________________\nrescaling (Rescaling)        (None, 256, 256, 3)       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 256, 256, 32)      896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 256, 256, 32)      0         \n_________________________________________________________________\ndropout (Dropout)            (None, 256, 256, 32)      0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 256, 256, 64)      18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128, 128, 64)      0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 128, 128, 128)     73856     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64, 64, 128)       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 64, 64, 192)       221376    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 32, 32, 192)       0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 32, 32, 192)       0         \n_________________________________________________________________\nflatten (Flatten)            (None, 196608)            0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               50331904  \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 50,687,745\nTrainable params: 50,687,745\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Sequential Model Architecture Setup \"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "\"\"\" CNN Layering Steps \"\"\"\n",
    "model.add(input_layer)\n",
    "\n",
    "#Data Augmentation\n",
    "model.add(resizing_layer)\n",
    "model.add(normalization_layer)\n",
    "\n",
    "model.add(conv_layer_1)\n",
    "model.add(pooling_layer_1)\n",
    "model.add(dropout_layer_1)\n",
    "\n",
    "\n",
    "model.add(conv_layer_2)\n",
    "model.add(pooling_layer_2)\n",
    "model.add(dropout_layer_2)\n",
    "\n",
    "model.add(conv_layer_3)\n",
    "model.add(pooling_layer_3)\n",
    "model.add(dropout_layer_3)\n",
    "\n",
    "model.add(conv_layer_4)\n",
    "model.add(pooling_layer_4)\n",
    "model.add(dropout_layer_4)\n",
    "\n",
    "model.add(flattening_layer)\n",
    "\n",
    "model.add(dense_layer_1)\n",
    "model.add(dropout_layer_5)\n",
    "model.add(dense_layer_2)\n",
    "model.add(dropout_layer_6)\n",
    "model.add(dense_layer_3)\n",
    "model.add(output_layer)\n",
    "\n",
    "\n",
    "\"\"\" CNN Architecture Summarization \"\"\"\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž Neural Network Configuration ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll compile your CNN architecture with appropriate parameters for loss calculation, optimization, and accuracy metrics.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for best-case parameters to use for image recognition models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" CNN Model Compilation \"\"\"\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Second change - adam to Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Predictive Fitness ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll be taking your compiled model and fitting it against your training and validation data.\n",
    "> \n",
    "> Keep in mind that there are several opportunities for further optimizing your workflow with techniques such as batch normalization, generator-based data feeding, etc. \n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for designing model fitness with validation data. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/9\n",
      "28/28 [==============================] - 155s 5s/step - loss: 0.8725 - accuracy: 0.5785 - val_loss: 0.6315 - val_accuracy: 0.7593\n",
      "Epoch 2/9\n",
      "28/28 [==============================] - 150s 5s/step - loss: 0.5474 - accuracy: 0.7575 - val_loss: 0.6231 - val_accuracy: 0.5324\n",
      "Epoch 3/9\n",
      "28/28 [==============================] - 147s 5s/step - loss: 0.5002 - accuracy: 0.8025 - val_loss: 0.3325 - val_accuracy: 0.8472\n",
      "Epoch 4/9\n",
      "28/28 [==============================] - 150s 5s/step - loss: 0.2815 - accuracy: 0.9088 - val_loss: 0.3322 - val_accuracy: 0.8611\n",
      "Epoch 5/9\n",
      "28/28 [==============================] - 146s 5s/step - loss: 0.2246 - accuracy: 0.9273 - val_loss: 0.1761 - val_accuracy: 0.9120\n",
      "Epoch 6/9\n",
      "28/28 [==============================] - 147s 5s/step - loss: 0.1706 - accuracy: 0.9480 - val_loss: 0.1596 - val_accuracy: 0.9213\n",
      "Epoch 7/9\n",
      "28/28 [==============================] - 147s 5s/step - loss: 0.1410 - accuracy: 0.9527 - val_loss: 0.1720 - val_accuracy: 0.9352\n",
      "Epoch 8/9\n",
      "28/28 [==============================] - 148s 5s/step - loss: 0.1500 - accuracy: 0.9503 - val_loss: 0.1519 - val_accuracy: 0.9306\n",
      "Epoch 9/9\n",
      "28/28 [==============================] - 150s 5s/step - loss: 0.0923 - accuracy: 0.9723 - val_loss: 0.1355 - val_accuracy: 0.9491\n"
     ]
    }
   ],
   "source": [
    "\"\"\" CNN Model Fitness and History Extraction \"\"\"\n",
    "#Third Change - number of epochs\n",
    "epochs = 9\n",
    "history = model.fit(train, validation_data = validation,\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### ðŸ”Ž CNN Model Evaluation ðŸ”\n",
    "\n",
    "---\n",
    "\n",
    "> For this task, you'll evaluate your CNN model using the validation dataset in order to calculate overall validation accuracy and loss.\n",
    "> \n",
    "> As always, refer to previous notebooks, tutorials, and documentation for using the proper evaluation function for model prediction. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7/7 [==============================] - 6s 869ms/step - loss: 0.1355 - accuracy: 0.9491\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.13546763360500336, 0.9490740895271301]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "\"\"\" CNN Model Predictive Evaluation \"\"\"\n",
    "model.evaluate(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”¹ Model State Saving\n",
    "\n",
    "When you are satisfied with your model state configuration and performance and are ready to export the model's weights and parameters for deployment purposes, simply run the following function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name, save_format):\n",
    "    \"\"\" \n",
    "    Save the model weights and architecture.\n",
    "    \n",
    "    Parameters: \n",
    "       model(Model): keras Model object being saved\n",
    "       file_name(str): name of the Hadoop file where\n",
    "                       the whole model will be saved\n",
    "       save_format(str): Indicates whether to save the model to the default\n",
    "                         SavedModel('tf'), or HDF5('h5'), or \n",
    "                         use both H5 and JSON ('composite') formats. \n",
    "       Returns: None\n",
    "    \"\"\"\n",
    "    MODEL_DIRECTORY = \"../model\"\n",
    "    def __save_as_composite():\n",
    "      \"\"\" Saving the model as H5 (for params) + JSON (for the architecture) \"\"\"\n",
    "      # Save the weights\n",
    "      model.save_weights(f'{MODEL_DIRECTORY}/{file_name}_params.h5')\n",
    "      # Save the architecture\n",
    "      with open(f'{MODEL_DIRECTORY}/{file_name}_layers.json', 'w') as f:\n",
    "          f.write(model.to_json())\n",
    "    \n",
    "    def __save_as_h5():\n",
    "      \"\"\" Option 2: Saving whole model as a single H5 file (more storage) \"\"\"\n",
    "      model.save(f\"{MODEL_DIRECTORY}/{file_name}.h5\", save_format=save_format)\n",
    "\n",
    "    # Call the appropiate save func\n",
    "    if save_format == 'h5':\n",
    "      __save_as_h5()\n",
    "    elif save_format == 'composite':\n",
    "      __save_as_composite()\n",
    "    else:  # save as a SavedModel\n",
    "      model.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"fire_cnn_classifier\", \"composite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go ahead and complete the remaining tasks in `project/PROJECT.md` to complete this project successfully! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "language": "python",
   "name": "python38864bitd38cb40033844df6909fab50e132a2aa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}